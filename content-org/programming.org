#+author: George Kettleborough
#+hugo_draft: t
#+hugo_base_dir: ../
#+hugo_categories: Programming
#+html_container: section
#+html_container_nested: t

* DONE Clean Domain Models with SQLAlchemy   :python:sqlalchemy:sql:databases:
CLOSED: [2024-12-08 Sun 21:42]
:PROPERTIES:
:EXPORT_FILE_NAME: clean-domain-models-sqlalchemy
:END:

** Introduction                                                      :ignore:

One of the most tried and tested techniques in programming is to build components that
map closely to the real-world problems you are trying to solve. In [[https://en.wikipedia.org/wiki/Domain-driven_design][domain-driven design]]
(DDD), one of the key tenets is to have a /domain model/ at the centre of your
application.

The domain model should be built in close collaboration with domain experts and,
crucially, should consist purely of high-level domain logic and be completely free of
low-level application logic like databases, GUIs, web frameworks etc.

For example, a publishing system might have an action called "publish" that makes sense
to all domain experts. An /implementation/ might ultimately be as simple as a SQL
~UPDATE~ on a table setting ~is_published=true~, but the domain model should not be
polluted with such details. "Publish" is part of the domain model; ~UPDATE~ is not.

Let's look at how we can use SQLAlchemy to handle database persistence of domain objects
without polluting our domain model.

** A domain model

The domain model is the heart of our application, so it makes sense to start with that.

#+begin_src python
class TodoList:

    def __init__(self, name: str, items: list[str]):
        self.name = name
        self.items = items

    def add_to_bottom(self, item: str) -> None:
        self.items.append(item)

    def add_to_top(self, item: str) -> None:
        self.items.insert(0, item)

    def get_first_item(self) -> str | None:
        if self.items:
            return self.items[0]
        return None

    @property
    def hashed_name(self) -> str:
        m = hashlib.sha256()
        m.update(self.name.encode())
        return m.hexdigest()
#+end_src

There are a few things to note about this ~TodoList~:

1. It has a simple attribute ~name~,
2. It has a complex attribute ~items~, this is a ~list~ of ~str~. Being a list means it
   is *ordered*,
3. It has an invariant property ~hashed_name~ which is always the sha256 hash of ~name~.

But, more importantly, note that this has absolutely no logic concerning databases or
persistence in any way.

Now let's use SQLAlchemy to persist these ~TodoList~ objects in a database. The goal is
to *not touch the domain model at all*. We are going to build a layer of application
logic which does not concern the domain experts and therefore it should not be part of
the domain model.

** Simple fields

Let's start with the simple attribute ~name~. We can easily define a database table that
has the single field ~name~, along with a database generated ~id~ column. Note that this
column is not part of our domain model, but is useful to generate anyway. The following
describes a database table using SQLAlchemy:

#+begin_src python
from sqlalchemy import Column, Integer, String, Table
from sqlalchemy.orm import registry

mapper_registry = registry()

todo_lists = Table(
    "todo_lists",
    mapper_registry.metadata,
    Column("id", Integer, primary_key=True, autoincrement=True),
    Column("name", String(), unique=True, nullable=False),
)
#+end_src

We now tell SQLAlchemy to map this table to our domain class:

#+begin_src python
mapper_registry.map_imperatively(TodoList, todo_lists)
#+end_src

By default SQLAlchemy maps columns names to attribute names, but you can tell it to do
otherwise (as we will later). What we've just set up is an /object-relational mapping/
(ORM). SQLAlchemy will map data to/from relations (in a SQL database) and object (in our
application).

Now we can persist an object in a database, but only the ~name~ field is saved:

#+begin_src python
engine = create_engine("sqlite:///test.db", echo=True)
mapper_registry.metadata.drop_all(engine)
mapper_registry.metadata.create_all(engine)

# construct domain object
my_list = TodoList("my_list", ["one", "two", "three"])

# save in database
with Session(engine) as s:
    s.add(my_list)
    s.commit()

# retrieve from database
with Session(engine) as s:
    l = s.get(TodoList, 1)

l.name    # => "my_list"
l.items   # => AttributeError
#+end_src

Now let's look at how to save the ~items~ field.

** Complex fields

The ~items~ attribute of ~TodoList~ is complex: it is made up of multiple parts. In
addition, it encodes important domain semantics, namely it is an /ordered/ list.

Some databases, like Postgres, support many complex field types, including arrays, which
would fit our purpose here quite nicely. But if we don't want to rely on a particular
database implementation we have to do something else.

In SQL the way to handle this is to use foreign keys and joins. What we need is another
table containing the todo items which has a foreign key to a todo list. That's easy
enough:

#+begin_src python
todo_items = Table(
    "todo_items",
    mapper_registry.metadata,
    Column("list_id", ForeignKey("todo_lists.id"), primary_key=True),
    Column("item_name", String(), primary_key=True),
)
#+end_src

Now, we need to create a class for SQLAlchemy to map to. This class won't be part of our
domain model, it exists purely so that SQLAlchemy can do its thing:

#+begin_src python
@dataclass
class TodoListItem:
    item_name: str
    list_id: int | None = None

mapper_registry.map_imperatively(TodoListItem, todo_items)
#+end_src

Note the ~list_id~ is optional because we won't have a value for this when we construct
a new ~TodoList~.

At this point, SQLAlchemy knows how to map this new ~TodoListItem~ class to the table
~todo_items~, but we don't want to use this class directly, we want to go through our
domain model. We need to tell SQLAlchemy how to handle this relationship by updating the
mapping for ~TodoList~:

#+begin_src python
mapper_registry.map_imperatively(
    TodoList, todo_lists,
    properties={
        "_items": relationship(
            TodoListItem,
            cascade="all, delete-orphan",
            lazy="selectin",
        ),
    },
)
#+end_src

This doesn't quite do what we want, this would map an attribute ~_items~ on our domain
model to those ~TodoListItem~ objects, which are not part of our domain model at
all. What we want is to extract just the ~item_name~ from the related table and map
those to our ~TodoList.items~ list. We can do that with an ~association_proxy~:

#+begin_src python
from sqlalchemy.ext.associationproxy import association_proxy

TodoList.items = association_proxy("_items", "item_name")
#+end_src

Now we can persist the items:

#+begin_src python
engine = create_engine("sqlite:///test.db", echo=True)
mapper_registry.metadata.drop_all(engine)
mapper_registry.metadata.create_all(engine)

# construct domain object
my_list = TodoList("my_list", ["one", "two", "three"])

# save in database
with Session(engine) as s:
    s.add(my_list)
    s.commit()

# retrieve from database
with Session(engine) as s:
    l = s.get(TodoList, 1)

l.name    # => "my_list"
l.items   # => ['one', 'three', 'two']
#+end_src

But hold on, what is going on with the order?! This is because we are still missing an
important part of the database representation. Not only are we forced to have separate
tables and foreign keys, we also must handle the order ourselves. SQL databases are
strictly unordered, unless an order is specified.

First let's amend our ~todo_items~ table to add a position:

#+begin_src python
todo_items = Table(
    "todo_items",
    mapper_registry.metadata,
    Column("list_id", ForeignKey("todo_lists.id"), primary_key=True),
    Column("item_name", String(), primary_key=True),
    Column("position", Integer()),
)
#+end_src

We should also add this to our ~TodoListItem~ class:

#+begin_src python
@dataclass
class TodoListItem:
    item_name: str
    list_id: int | None = None
    position: int | None = None
#+end_src

Now we can tell SQLAlchemy to order the relation by specifying
~order-by="TodoListItem.position"~ but how do we write those positions in the first
place? Fortunately there is special collection type called ~ordering_list~ which will
handle this for us:

#+begin_src python
mapper_registry.map_imperatively(
    TodoList, todo_lists,
    properties={
        "_items": relationship(
            TodoListItem,
            order_by="TodoListItem.position",
            collection_class=ordering_list("position"),
            cascade="all, delete-orphan",
            lazy="selectin",
        ),
    },
)
#+end_src

Now when we write an object to the database, ~ordering_list~ will automatically fill in
the ~position~ column for us according to the order of the list in the domain
object. When we retrieve an object from the database the list will be ordered according
to those ~positions~ again.

We've now managed to persist a complex field in the database without polluting our
domain model with anything at all. As it happens we needed to create a new table, a
foreign key and an ordering column, but our domain model is none the wiser! It's still
just a list.

Next let's see how we can persist that generated ~hashed_name~ field in the database.

** Generated fields

Our domain model has an invariant: the ~hashed_name~ is always the sha256 of the
~name~. If we want to be able to search for this hash efficiently we will need to
persist it in the database. What we'd like is to write this field to the database when
the object is created or updated.

First, let's add it to the table definition:

#+begin_src python
todo_lists = Table(
    "todo_lists",
    mapper_registry.metadata,
    Column("id", Integer, primary_key=True, autoincrement=True),
    Column("name", String(), unique=True, nullable=False),
    Column(
        "hashed_name", String(), index=True, nullable=False, key="_hashed_name"
    ),
)
#+end_src

Note we set ~key="_hashed_name"~. This causes SQLAlchemy to map it to/from a hidden
field on model ~_hashed_name~, rather than try to set the property, which it can't.

In order to update this value according to our domain model we can set some triggers:

#+begin_src python
@event.listens_for(TodoList, "before_insert")
@event.listens_for(TodoList, "before_update")
def populate_hashed_name(mapper, connection, target):
    target._hashed_name = target.hashed_name
#+end_src

Now we can persist the whole thing and retrieve by hash:

#+begin_src python
engine = create_engine("sqlite:///test.db", echo=True)
mapper_registry.metadata.drop_all(engine)
mapper_registry.metadata.create_all(engine)

# construct domain object
my_list = TodoList("my_list", ["one", "two", "three"])

# save in database
with Session(engine) as s:
    s.add(my_list)
    s.commit()

# retrieve from database
with Session(engine) as s:
    result = s.execute(select(TodoList).where(
        TodoList._hashed_name == "495a613093452715b9989b8233829836804bce4c1f95e221f86da526ea93281b"
    ))
    for obj in result.scalars():
        print(obj.name)         # => 'my_list'
#+end_src

** Conclusion

So now we are able to persist our domain model fully into the database. SQLAlchemy does
its job as an ORM to make this mapping complete. From the point of view of our domain
model the ~TodoList~ is just a Python class and the business rules can be expressed and
tested in regular Python code.

The full code is here:

#+begin_src python
from dataclasses import dataclass
import hashlib

from sqlalchemy import (
    Column,
    ForeignKey,
    Integer,
    String,
    Table,
    create_engine,
    event,
)
from sqlalchemy.ext.associationproxy import association_proxy
from sqlalchemy.ext.orderinglist import ordering_list
from sqlalchemy.orm import registry, relationship

# --- Domain ---

class TodoList:
    """The domain model for a todo list"""

    def __init__(self, name: str, items: list[str]):
        self.name = name
        self.items = items

    def __eq__(self, other) -> bool:
        if isinstance(other, TodoList) and self.name == other.name:
            return True
        return False

    def __hash__(self) -> int:
        return hash(self.name)

    def add_to_bottom(self, item: str) -> None:
        self.items.append(item)

    def add_to_top(self, item: str) -> None:
        self.items.insert(0, item)

    def get_first_item(self) -> str | None:
        if self.items:
            return self.items[0]
        return None

    @property
    def hashed_name(self) -> str:
        m = hashlib.sha256()
        m.update(self.name.encode())
        return m.hexdigest()


# --- ORM stuff ---

mapper_registry = registry()

todo_lists = Table(
    "todo_lists",
    mapper_registry.metadata,
    Column("id", Integer, primary_key=True, autoincrement=True),
    Column("name", String(), unique=True, nullable=False),
    Column("hashed_name", String(), index=True, nullable=False, key="_hashed_name"),
)

todo_items = Table(
    "todo_items",
    mapper_registry.metadata,
    Column("list_id", ForeignKey("todo_lists.id"), primary_key=True),
    Column("item_name", String(), primary_key=True),
    Column("position", Integer()),
)

@dataclass
class TodoListItem:
    item_name: str
    list_id: int | None = None
    position: int | None = None

mapper_registry.map_imperatively(TodoListItem, todo_items)

mapper_registry.map_imperatively(
    TodoList, todo_lists,
    properties={
        "_items": relationship(
            TodoListItem,
            order_by="TodoListItem.position",
            collection_class=ordering_list("position"),
            cascade="all, delete-orphan",
            lazy="selectin",
        ),
    },
)

TodoList.items = association_proxy("_items", "item_name")

@event.listens_for(TodoList, "before_insert")
@event.listens_for(TodoList, "before_update")
def populate_hashed_name(mapper, connection, target):
    target._hashed_name = target.hashed_name
#+end_src

In an ORM like Django, we would be forced to bend our model to the needs of the
database, like having a foreign key relationship for ~items~, and would have to pollute
our model with ORM specific stuff like column types etc. When it comes to testing, you
end up needing a database the moment you have a relationship (or the complex field in
our example).

Alternatively we could consider Django models to be just database tables and manually
map them to domain models ourselves. But in that case we'd also have to implement the
unit of work pattern ourselves and track the changes to each object so we know which
ones to update. But isn't that what the ORM is for? Django seems to only do half the
job. SQLAlchemy does this for us, of course.

One thing you might be wondering is whether it's true that SQLAlchemy didn't touch the
domain model. What are those ~_items~ and ~_hashed_name~ attributes? And what about this
~_sa_instance_state~ that you'll see on instances from the db? SQLAlchemy does indeed
dynamically modify the instances to keep track of changes and implement some of the
magic. You do have to take care when setting up the mapping, but it should always be the
mapping that bends to the needs of the domain model, not the other way around.

If done properly the mapping won't affect the way the domain model operates in any
way. You could instantiate an instance either via SQLAlchemy or its ~__init__~ method,
or perhaps by a special test repository that doesn't use a database. It will all be the
same. But that doesn't mean an end-to-end test or two wouldn't be appropriate.

Finally, I did wonder about using a ~deque~ for the ~items~ instead of a ~list~. After
all, ~self.items.insert(0, item)~ is not an efficient operation for a list (nor would a
~pop_first_item~ method, for example). Using a ~deque~ isn't quite so easy. SQLAlchemy
includes the machinery for ~list~, ~set~ and ~dict~, but you would have to provide your
own ~proxy_factory~ argument to ~association_proxy~ to use other collections. This is
possible, though, if you need it.

* TODO A Test Needs to Fail
:PROPERTIES:
:EXPORT_FILE_NAME: a-test-needs-to-fail
:END:

** Introduction                                                      :ignore:

If you write software for a codebase with a test suite, it might sometimes seem like the
point of a test is to pass. After all, passing tests are /good/; they indicate you've
done your job properly. And they're coloured green!

Many developers seem to work under this assumption. After adding a new feature they'll
run the tests. If they all pass then great, job done, if any fail they then proceed to
fix the code and/or the tests themselves. Only then do they proceed to add new tests for
the functionality they've just added.

But this is missing the whole point of a test: the point of a test, it's /raison
d'être/, is to *fail*. If you follow the workflow above you'll never see your test
fail. This is a very, very bad thing because you might have written a test that *cannot
fail*. Such a test is worse than useless, it's actively harmful as it bloats the
codebase, wastes computing time, and offers false reassurance to anyone running the
test suite. This happens all the time.

** Test-driven development

This is why, under test-driven development, the first step is always to write the test
and see it fail. This is the only way to know you've written a test that can fail. If
you've already implemented the feature it's already too late, you'll never see it fail.

Unfortunately, we all know it's easy to get carried away and write the (fun)
implementation before the (boring) test. If you do find yourself in this situation,
consider stashing[fn:1] the feature then writing the test before unstashing it to
(hopefully) see it pass. However, if it doesn't pass the tendency might still be to
alter the test to make it pass. That's still writing a passing test and not following
TDD.

Note that one common misconception about TDD is it's about unit tests. TDD is not about
unit tests.[fn:2] The most important thing here is the red-green-refactor workflow of
TDD.

** Fixing bugs

Writing a test first is especially important when fixing a bug. A bug always means there
is a gap in your test suite. The existing code passed despite having the bug. So fill
the gap in your tests first: write the missing (failing) test! These are often the most
important tests because they test non-obvious behaviour that is easy to get
wrong. Writing a passing test afterwards can be even worse than not writing a test at
all!

** Conclusion



[fn:1] https://git-scm.com/docs/git-stash

[fn:2] https://www.youtube.com/watch?v=IN9lftH0cJc

* TODO Falling Out of Love with Django
:PROPERTIES:
:EXPORT_FILE_NAME: falling-out-of-love-with-django
:END:

I started web programming in the early 2000s with PHP. I'd already learnt HTML as a
child and PHP gave me the ability to generate HTML dynamically from a database. I built
my own blog and did my first paid work developing sites for local businesses. But I
started to see that I was repeating the same patterns over and over again: get record
from database, map fields to HTML elements, reverse that for forms etc. You get the
idea.

At that time I didn't know about "libraries" or "frameworks". They might have existed, I
honestly don't know, but I started to see a need for them anyway. But it didn't matter
because I was just about to start university and stop web programming, at least for a
while.

It wasn't until 2018 that I decided to resurrect my atrophied web skills for a project
at work. I wanted to build a network application for a team to collaboratively build a
dataset and a web-based, database-driven application seemed like the way to go. I might
have given PHP a passing thought but I'd been using Python for a while at that point and
I quite liked it, so I discovered Django.

I very quickly fell in love with Django. It was everything I was beginning to imagine
back in the PHP days. To a complete newbie it surely seems like magic, but I knew what
it was doing underneath and was very happy to let it take the drudgery out of building
CRUD applications.

Later I would join a company that used Django heavily for business software that handled
day to day operations. I didn't even know this when I joined the company so I jumped on

** Django models are not models

Dictionary (OED):

#+begin_quote
model: A simplified description of a system, process etc. put forward as a basis for
theoretical or empirical understanding; a conceptual or mental representation of
something.
#+end_quote

Django models have a direct relationship with database tables. They follow the "active
record" pattern. This coupling to the database means they are severely lacking for all
but the simplest domain modelling requirements.

A simple example is a model with a compound field. Imagine this basic model of a todo
list:

#+begin_src python
class TodoList:
    def __init__(self, name):
        self.name = name
        self.items = []

    def add_item(self, item: str) -> None:
        self.items.append(item)

    def get_items(self) -> list[str]:
        return self.items

    def complete_item(self, item: str) -> None:
        self.items.remove(item)
#+end_src

Seems like a pretty basic model. But we can't do this in Django. We'd have to do
something like this:

#+begin_src python
class TodoList(models.Model):
    name = models.CharField()


class TodoListItem(models.Model):
    todo_list = models.ForeignKey(TodoList, on_delete=models.CASCADE)
    order = models.PositiveIntegerField()
    content = models.TextField()
#+end_src

What the hell is this? Foreign keys? Two classes? This is an insane way to model a todo
list, but this is what we are forced to do in Django.

Can't validate an aggregate before saving it to the database.

Can't test a model without having a database. I can't instantiate a ~TodoList~, add some
items to it, and run some tests. I am forced to persist it and the related objects into
the database before I can test it.

** Django views are not views



** Django templates suck

* DONE Working on Multiple Web Projects with Docker Compose and Traefik :networking:web:development:traefik:docker:
CLOSED: [2023-10-02 Mon 09:00]
:PROPERTIES:
:EXPORT_FILE_NAME: multiple-web-projects-traefik
:EXPORT_HUGO_LASTMOD: [2023-10-19 Thu 21:24]
:END:

** Introduction                                                      :ignore:

Docker Compose is a brilliant tool for bringing up local development environments for
web projects.  But working with multiple projects can be a pain due to clashes.  For
example, all projects want to listen to port 80 (or perhaps one of the super common
higher ones like 8000 etc.).  This forces developers to only bring one project up at a
time, or hack the compose files to change the port numbers.

Recently I've found a way that makes managing these more enjoyable.

/2023-10-05 note: If this interesting to you, be sure to check out the comments about this
article on [[https://news.ycombinator.com/item?id=37756632][Hacker News]] for many other ideas./

/2023-10-19 note: I have now created a repo formalising the ideas in this post and some
of the Hacker News comments, here: https://github.com/georgek/traefik-local/

** A single project with Docker Compose

I use [[https://docs.docker.com/compose/][docker compose]] to manage local development instances of these projects.  A typical
compose file for a web project might look like this:

#+begin_src yaml
# proj/compose.yaml
services:
  db:
    image: "postgres"
    environment:
      POSTGRES_DB: "proj"
      POSTGRES_USER: "user"
      POSTGRES_PASSWORD: "pass"

  web:
    build: .
    depends_on:
      - "db"
    environment:
      DATABASE_URL: "postgres://user:pass@db/proj"
    ports:
      - "8000:80"
#+end_src

Note the very last line.  This is where we map port 8000 from the host to port 80 of the
container such that the service can be accessed via ~http://127.0.0.1:8000~.

This works quite well for a single project, but it suffers from a couple of problems if
you work on multiple projects:

1. It doesn't scale.  If I want to run another project at the same time, I'll have to
   use a different port number, maybe 8001, then 8002 etc.,

2. What if that ~compose.yaml~ file is checked in as part of the project? Does the whole
   team have to agree on a set of port numbers to use for each project?

** Using overrides for multiple projects

Fortunately Docker Compose does have a solution for (2) in the form of the
~compose.override.yaml~ file.  This file will be automatically be [[https://docs.docker.com/compose/multiple-compose-files/merge/][merged]] into the
~compose.yaml~ without any extra configuration.

Unlike some other guides (including the official [[https://docs.docker.com/compose/multiple-compose-files/merge/#example][docs]]) concerning this file, I prefer to
*not* check ~compose.override.yaml~ into version control and instead add it to the
~.gitignore~ file. Adding it to version control completely defeats the purpose of it: to
allow individual developers to override the standard compose file.

So, with this in mind, I no longer expose any ports by default in ~compose.yaml~ because
I don't know what will be convenient for each developer.  This set up might look like
this:

#+begin_src yaml
# compose.yaml
services:
  db:
    image: "postgres"
    environment:
      POSTGRES_DB: "proj"
      POSTGRES_USER: "user"
      POSTGRES_PASSWORD: "pass"

  web:
    build: .
    depends_on:
      - "db"
    environment:
      DATABASE_URL: "postgres://user:pass@db/proj"
#+end_src

#+begin_src yaml
# compose.override.yaml (to be created by each developer)
services:
  web:
    ports:
      - "8000:80"
#+end_src

** Using Traefik

So now each developer can pick their own port numbers for each project, but we can still
do better than this.  People aren't good at remembering numbers.  We are much better at
remembering names.  [[https://doc.traefik.io/traefik/][Traefik]] is a free software edge router that can be used as a simple
and super easy to configure reverse-proxy in container-based set ups.

Using Docker, Traefik can automatically discover services to create routes to.  It uses
container labels to further configure these routes.  The following tiny example from the
[[https://doc.traefik.io/traefik/getting-started/quick-start/][docs]] is illustrative:

#+begin_src yaml
# traefik/compose.yaml
services:
  reverse-proxy:
    image: traefik:v2.10
    ports:
      - "80:80"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
  whoami:
    image: traefik/whoami
    labels:
      - "traefik.http.routers.whoami.rule=Host(`whoami.docker.localhost`)"
#+end_src

This starts two containers on the same docker network.  The reverse proxy listens on
port 80 and forwards traffic with a host header of "whoami.docker.localhost" to the
~whoami~ service.  Traefik guesses which port to send it to ~whoami~ based on the ports
exposed by the container.

If you haven't played with Traefik before it's worth going through the [[https://doc.traefik.io/traefik/getting-started/quick-start/][quick-start]]
properly now then coming back to see how we can make this work for multiple projects.

** Traefik with multiple projects

This doesn't quite solve our problem yet.  We don't want all of our various projects
inside one compose file.  Luckily Traefik communicates with the Docker daemon directly
and doesn't really care about the compose file, but you do need to make sure a few
things are in order for this to work.

Firstly, make a docker network especially for Traefik to communicate with other services
that you want to expose, for example:

#+begin_src yaml
# traefik/compose.yaml
services:
  reverse-proxy:
    image: traefik:v2.10
    restart: unless-stopped
    command: --api.insecure=true --providers.docker
    ports:
      - "80:80"
      - "8080:8080"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks:
      - traefik

networks:
  traefik:
    attachable: true
    name: traefik
#+end_src

We create the network ~traefik~ and give it the name "traefik" (otherwise docker compose
would scope it by project, e.g. "traefik_traefik").  We also allow other containers to
attach to this network.

Then in our ~compose.override.yaml~ file from above, instead of mapping ports, we do the
following:

#+begin_src yaml
# proj/compose.override.yaml
services:
  web:
    labels:
      - "traefik.http.routers.proj.rule=Host(`proj.traefik.me`)"
      - "traefik.http.services.proj.loadbalancer.server.port=8000"
      - "traefik.docker.network=traefik"
    networks:
      - default
      - traefik

networks:
  traefik:
    external: true
#+end_src

Now, after bringing up first the traefik project then your web project, you should be
able to browse to [[http://proj.traefik.me/]] in your web browser.

There's a few things going on here.  First, we have declared the ~traefik~ network as an
external network.  This means compose won't manage it, but expects it to exist (so you
must start your traefik composition first).  Next we override the ~networks~ setting of
~web~ to make it part of the ~traefik~ network too.  Note we also have to add the
~default~ network, otherwise it wouldn't be able to communicate with ~db~ and other
services on its own default network.

The ~traefik.http.routers.proj.rule~ label configures Traefik to route HTTP traffic with
the "proj.traefik.me" hostname to the container. The ~traffic.docker.network~ label is
necessary because ~web~ is on two networks.  Finally, we set
~traefik.http.services.proj.loadbalancer.server.port~ for completeness, just in case
your container needs a different port mapping than the port it is set to expose, or if
it exposes multiple ports.

There is one final piece of magic: the "traefik.me" hostname.  What is that?  You can
read about it at [[http://traefik.me/]].  Essentially it is a DNS service that resolves to
any IP address that you want, but by default it resolves ~<xxx>.traefik.me~ to
~127.0.0.1~.  There are other services like this including [[https://sslip.io/]] and
[[https://nip.io/]].

Now, because we don't need to define any ports at all, it is possible to take advantage
of a newish compose feature and reinstate the ports in the original ~compose.yaml~ file
for those developers who don't want to set up Traefik for themselves.  So our final
configuration looks like this:

#+begin_src yaml
# compose.yaml
services:
  db:
    image: "postgres"
    environment:
      POSTGRES_DB: "proj"
      POSTGRES_USER: "user"
      POSTGRES_PASSWORD: "pass"

  web:
    build: .
    depends_on:
      - "db"
    environment:
      DATABASE_URL: "postgres://user:pass@db/proj"
    ports:
      - "8000:80"
#+end_src

#+begin_src yaml
# compose.override.yaml (to be created by each developer)
services:
  web:
    labels:
      - "traefik.http.routers.proj.rule=Host(`proj.traefik.me`)"
      - "traefik.http.services.proj.loadbalancer.server.port=8000"
      - "traefik.docker.network=traefik"
    networks:
      - default
      - traefik
    ports: !reset []

networks:
  traefik:
    external: true
#+end_src

The ~!reset []~ tag sets the ports back to empty; you can read about it [[https://docs.docker.com/compose/compose-file/13-merge/#reset-value][here]].  Note that
unfortunately it can't be used to set /new/ ports, only reset them to default (you would
have to use two layers of override file to set new ports).  The ~!reset~ tag requires a
fairly recent version of docker compose, at least greater than 2.18.0.

A final note: you can check that these overrides are working correctly by running
~docker compose config~.

** Conclusion

By leveraging both the ~compose.override.yaml~ file and Traefik it's easy to run
multiple web projects on your development system at the same time and have easy to
remember names to access them all.  Each developer is free to run as many as they want
and create their own easily-manageable configurations.  Traefik and traefik.me can also
be used to allow other developers on your network to easily access your local
development instances with no DNS configuration required.

It's a shame that the docs instruct people to use the override file for a distributed
developer config rather than let individual developers use it, but hopefully it's not
too hard to remove this file from repos if already present.

* DONE My 2023 Emacs Python Setup                              :emacs:python:
CLOSED: [2023-08-15 Tue 14:19]
:PROPERTIES:
:EXPORT_FILE_NAME: emacs-python-2023
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :description My new configuration with Emacs 29, Eglot, python-lsp-server and tree-sitter
:END:

** Introduction                                                      :ignore:

I've been using Emacs for almost 15 years now.  Somewhat surprisingly, I hadn't touched
my config in three years!  It's been working that well.  But now that Emacs 29 has been
released I've decided to take a look at what's new and there have been some big changes,
particularly for Python.

** Goodbye Elpy, Goodbye Projectile

[[https://github.com/jorgenschaefer/elpy/][Elpy]] has been the primary mode for Python development for me for years now.  But sadly,
it looks like the project is no more.  The good news is there are better ways to do what
it did.  It's bittersweet to say goodbye to it and I will be eternally grateful to the
authors, but progress is progress.

Similarly, [[https://github.com/bbatsov/projectile][Projectile]] was what I used to manage projects.  But now Emacs has project.el
built in and I've opted to use that instead.  One nice thing about project.el is it uses
other standard stuff underneath like xref.  I configured xref to use [[https://github.com/BurntSushi/ripgrep][Ripgrep]] and now the
Project commands like ~C-x p g~ use Ripgrep:

#+begin_src elisp
(use-package xref
  :config
  (setq xref-search-program 'ripgrep))
#+end_src

** Native builds and tree-sitter

I always build Emacs myself from source if I can.  I run Gentoo on my personal computer
so that goes without saying, but I do it on Ubuntu too, if only to get the latest
versions.  This does mean I can easily enable a couple of new features: native builds
and tree-sitter.

On Gentoo this was a simple as adding a couple of USE flags to portage.  My USE flags
for emacs now look like:

#+begin_src
app-editors/emacs athena cairo gui gtk harfbuzz json libxml2 source tree-sitter jit -X
#+end_src

The ~gtk -X~ also implies a ~pgtk~ build which is nice because I use wayland (sway).

On Ubuntu (20.04, yeah, old, this is one reason I prefer rolling distros) it was more
difficult.  I first pulled the source code (~emacs-29.1.tar.gz~) from a [[http://ftpmirror.gnu.org/emacs/][nearby GNU
mirror]] per the [[https://www.gnu.org/software/emacs/download.html][GNU website]].  Then a few packages are required (I use i3/X11 on
Ubuntu):

#+begin_src bash
sudo apt install autoconf make gcc texinfo libgtk-3-dev libxpm-dev libjpeg-dev \
     libgif-dev libtiff5-dev libgnutls28-dev libncurses5-dev libjansson-dev \
     libharfbuzz-dev libharfbuzz-bin imagemagick libmagickwand-dev libgccjit-10-dev \
     libgccjit0 gcc-10 libjansson4 libjansson-dev xaw3dg-dev texinfo libx11-dev
#+end_src

Now, because ~libgccjit~ (required for native builds) is only for GCC 10, the build
process has to be configured for GCC 10 specifically, in addition to enabling all the
wanted features:

#+begin_src bash
CC="gcc-10" ./configure --prefix=$HOME --with-json --with-native-compilation=aot \
  --with-modules --with-compress-install --with-threads --with-included-regex \
  --with-x-toolkit=lucid --with-zlib --with-jpeg --with-png --with-imagemagick \
  --with-tiff --with-xpm --with-gnutls --with-xft --with-xml2 --with-mailutils \
  --with-tree-sitter
#+end_src

Note that I keep my own builds in ~$HOME~ by setting ~--prefix~.  By default the
installation would put it in the system directories which I prefer not to do as those
are controlled by my system package manager.  Also note that I set
~--with-native-compilation=aot~ which makes native builds ahead of time instead of JIT
compiling them.  Run ~./configure --help~ to see all of the build options.

Then I just compiled it:

#+begin_src bash
make -j 8                       # 8 threads
#+end_src

The build can be tested with ~src/emacs -Q~ then, if it works:

#+begin_src bash
make install
#+end_src

** Eglot

Elpy provided a proper IDE experience for Python but it did it in a completely custom,
albeit very clever, way via a special RPC process which used ~jedi~.  Now with LSP we
can get essentially the same sort of thing but in a more standard way that works with
all languages.

I have tried LSP (in particular, [[https://emacs-lsp.github.io/lsp-mode/][~lsp-mode~]]) in emacs before, but I wasn't impressed.  I
cannot stand latency and the moment I detect latency when merely typing in a text
editor, I walk away.  But I'm pleased to say that with Emacs 29, native builds, Eglot
and [[https://github.com/python-lsp/python-lsp-server][~python-lsp-server~]] it is now fast enough for me.  ~lsp-mode~ might very well be
fast enough now too.  I'll probably try it eventually.

I installed ~python-lsp-server~ (with [[https://github.com/pypa/pipx][~pipx~]] on Ubuntu).  This is my preferred way of
installing Python apps if they're not available in the base distro.  Notice how there
will be only one LSP server installed for my whole system (not one per virtualenv).

Enabling Eglot is easy.  To make it work for Python it just needs the following:

#+begin_src elisp
(use-package eglot
  :hook (python-mode . eglot-ensure))
#+end_src

Now just open a Python file and it should work.  It does everything Elpy did (or, at
least, what I used it for) and more.  Just like that.

By default, Eglot uses Flymake.  I had previously been using Flycheck.  I can't really
remember why, to be honest, so I'll try using Flymake instead and say goodbye to
Flycheck for now too.

** Virtualenvs

OK, so, it doesn't completely just work.  One of the most important things for me is
being able to jump to the definition of a symbol in the source.  This does just work for
first party stuff and (kinda) for standard library stuff, but it won't work for third
party stuff.  That's because the LSP server doesn't know where to find those libraries.

Usually when developing on a Python project one would create a virtual environment for
it.  I make everything a package such that doing a ~pip install -e .~ installs the
package and all of its dependencies into the virtualenv.  Then you just need to make the
LSP server aware of this environment.

I used to use ~virtualenvwrapper~ to create virtualenvs for each project, but I've found
a better way: [[https://direnv.net/][~direnv~]].  This allows you to create ~.envrc~ files in directories with
anything you want in it then automatically loads it into your environment when you
change to that directory.  What's even neater is it has built-in support for Python (and
other languages).

To install ~direnv~ on Gentoo I used the [[https://github.com/gentoo-mirror/guru][Guru]] overlay:

#+begin_src bash
eselect repository enable guru
#+end_src

After installing and setting up ~direnv~, make a file called ~.envrc~ at the top of your
project and put the following:

#+begin_src bash
layout python
#+end_src

That's it!  After enabling your project for ~direnv~ support it will automatically
create a virtualenv and activate it.  When you change directory, it will deactivate it.
Amazing!

In Emacs you can install the ~direnv~ package and enable it:

#+begin_src elisp
(use-package direnv
  :config
  (direnv-mode))
#+end_src

Now when you browse to a project with a ~.envrc~ file it will just work.

** Tree-sitter

Finally, to enable tree-sitter I needed to first install the grammar for Python, I added
the following to my emacs config:

#+begin_src elisp
(setq treesit-language-source-alist
   '((python "https://github.com/tree-sitter/tree-sitter-python")))
#+end_src

And then (after evaling the above) you can run: ~M-x treesit-install-language-grammar~.
This builds the grammar for you and puts it in your emacs config.

Now you can use the mode ~python-ts-mode~ instead of ~python-mode~:

#+begin_src elisp
(use-package python
  :mode ("\\.py\\'" . python-ts-mode)
  :init
  (add-to-list 'major-mode-remap-alist '(python-mode . python-ts-mode)))
#+end_src

The ~major-mode-remap-list~ entry means ~python-ts-mode~ will be used whenever
~python-mode~ would have been used, like when opening a script with no file extension
but a Python shebang.

** Completion

One thing I cannot do without is some kind of completion capability.  In bash I use
tab-completion extensively and I consider any keyboard-driven software that doesn't
support at least tab-completion to be defective.

Basic completion is supported in Emacs out of the box but it can be extended to be quite
sophisticated.  But I've always found it a bit overwhelming.  My life was changed when I
first enabled [[https://www.gnu.org/software/emacs/manual/html_mono/ido.html][ido]].  The combination of completion and narrowing is amazing.  Later I
switched to other packages like [[https://github.com/abo-abo/swiper][ivy]], [[https://emacs-helm.github.io/helm/][Helm]] and [[https://github.com/radian-software/selectrum][Selectrum]] and enabled in-buffer completion
with [[https://company-mode.github.io/][Company]].  Selectrum is now defunct and replaced with [[https://github.com/minad/vertico][Vertico]].

For the first time, I have a completion set up that I understand and that I'm very happy
with.

What I really wanted was fuzzy-style completion in minibuffer contexts but dead basic
prefix-style completion within buffers.  I also want the completion within-buffer to be
driven by the tab key like in a bash shell.  I've settled on Company within-buffer and
Vertico in the minibuffer.

I like the setting ~(setq tab-always-indent 'complete)~ which causes TAB to indent
first, then complete, but I was getting weird behaviour where that completion would not
launch company.  So instead:

#+begin_src elisp
(global-set-key (kbd "TAB") #'company-indent-or-complete-common)
#+end_src

This now does the right thing but launches Company instead of the default completion
function.

The other major part is completion styles.  I like the [[https://github.com/oantolin/orderless][Orderless]] style for the fuzzy
minibuffer style, but it doesn't work for basic completion.  Emacs supports setting a
list of completion styles by setting ~completion-styles~ and further refining that for
specific categories by setting ~completion-category-overrides~.  The trouble is, the
category names for the latter are quite hard to find.  But eventually I settled on the
following configuration:

#+begin_src elisp
(use-package orderless
  :init
  (setq completion-styles '(basic partial-completion orderless)
        completion-category-defaults nil
        completion-category-overrides '((project-file (styles orderless))
                                        (buffer (styles orderless))
                                        (command (styles orderless)))))
#+end_src

This sets ~basic~ and ~partial-completion~ styles first by default everywhere.  Company
doesn't really support the Orderless style, which is fine by me as I don't want it
in-buffer anyway.  I then override it for particular categories to add ~orderless~ to
the front.  ~project-file~ is for finding files in projects with ~C-x p f~, ~buffer~ is
for switching buffers and ~command~ is for running commands with ~M-x~.

** Conclusion

To sum up, I've switched from Projectile to project.el, from Elpy to Eglot/LSP and from
virtualenvwrapper to direnv as well as including the latest improvements like native
builds and tree-sitter.  This has really simplified my config and I seem to have a
renewed love for Emacs.

I've been using this configuration for a few days now for real work and I really love it
so far.  Things like the eldoc and xref jump to definition features are working
perfectly now and I've had real trouble with consistent behaviour before.

My actual emacs config does include a number of extra tweaks to all of this stuff.  I
love reading other people's .emacs files, so maybe you'll enjoy reading mine too:
https://github.com/georgek/dot-emacs

Happy hacking!

* DONE Arduino Programming with Emacs :emacs:arduino:programming:electronics:
CLOSED: [2023-07-31 Mon 19:30]
:PROPERTIES:
:EXPORT_FILE_NAME: emacs-arduino
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :summary Develop for Arduino in your favourite text editor with PlatformIO
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :description An easy way to start Arduino for Emacs users
:END:

** Introduction                                                      :ignore:

If you want to start Arduino programming you'll notice a lot of the documentation and
tutorials are centred around the Arduino IDE.  Now, obviously, as an Emacs user you'll
be loath to install something like Arduino IDE, let alone actually use it.  The good
news is it's super easy to get started with Arduino with any editor, including Emacs and
even Vim if you so desire.

All the Arduino IDE is doing is calling a cross-compiler toolchain then using [[https://github.com/avrdudes/avrdude][~avrdude~]]
to communicate with the Arduino to upload software.  The Arduino Uno and Nano both use
the Atmel AVR platform so what you need is a toolchain that can target that platform.
Now, you could install your own toolchain and call ~avrdude~ directly.  If you know how
to do that then I guess you can stop reading now.  But if you don't, or aren't
interested in learning how (it's not very interesting), then read on.

** PlatformIO

[[https://platformio.org/][PlatformIO]] is a project that makes it really easy to do embedded development.

First, install PlatformIO, I like to use [[https://github.com/pypa/pipx][pipx]] to install tools like this: ~pipx install
platformio~.

Now, start your project by making a directory for it:

#+BEGIN_SRC sh
mkdir my_new_project
cd my_new_project
#+END_SRC

And initialise a PlatformIO project:

#+BEGIN_SRC sh
platformio project init --board uno --board nanoatmega328
#+END_SRC

This will configure your project for both Arduino Uno and Nano.

Now write some barebones C++ code that does nothing in ~src/main.cpp~:

#+BEGIN_SRC cpp
#include "Arduino.h"

void setup()
{
    // your setup code here
}

void loop()
{
    // your main loop here
}
#+END_SRC

This is, of course, totally standard C++ so you can use your normal C++ modes etc.

You should end up with a project structure like this:

#+BEGIN_SRC text
.
├── include
│   └── README
├── lib
│   └── README
├── platformio.ini
├── src
│   └── main.cpp
└── test
    └── README
#+END_SRC

Now you can simply run the following to build the software for all platforms specified
in ~platformio.ini~:

#+BEGIN_SRC sh
platformio run
#+END_SRC

To build /and/ upload the software to your Arduino, if you are on Linux you first have
to install some udev rules:
https://docs.platformio.org/en/latest/core/installation/udev-rules.html

Then you can run simply:

#+BEGIN_SRC sh
platformio run -e nanoatmega328 -t upload # for arduino nano
platformio run -e uni -t upload # for arduino uno
#+END_SRC

This tends to cleverly pick the right serial device but if you have more than one you
might need to specify it with [[https://docs.platformio.org/en/latest/core/userguide/cmd_run.html#cmdoption-pio-run-upload-port][~--upload-port~]].

You can adapt these as your command for ~M-x compile~ or write a ~Makefile~ if you
prefer.  Don't forget it expects to be run from the top-level where ~platformio.ini~
lives, though.

Another super-useful command to be aware of is ~platformio device monitor~.  This gives
you a serial terminal for communicating with your device.  Really convenient.  There's a
lot more too.

And that's it!  You'll find the Arduino documentation here:
https://www.arduino.cc/reference/en/ That's all you should need to get started.  Happy
hacking!

# Local Variables:
# org-footnote-section: nil
# End:
